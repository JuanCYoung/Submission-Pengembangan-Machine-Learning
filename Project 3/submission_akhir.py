# -*- coding: utf-8 -*-
"""submission_akhir.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-L7OgF4K186hjqyQN_yysKmksBrjiJYw

### Klasifikasi Tumor Otak
Sumber Dataset : https://www.kaggle.com/datasets/sabersakin/brainmri
"""

pip install --upgrade tensorflow

!pip install -q kaggle

pip install split-folders

from google.colab import files
uploaded = files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

!kaggle datasets download -d sabersakin/brainmri

"""### Import Library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from google.colab import files
import tensorflow as tf
import os
import zipfile
import re, string
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
import tensorflow_hub as hub
import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib.cm as cm
import matplotlib.image as mpimg
import splitfolders
from pathlib import Path
import os.path
import shutil
import random

"""### Load Data"""

zipPath = '/content/brainmri.zip'
zipFile = zipfile.ZipFile(zipPath,'r')
zipFile.extractall('/content/BrainMRI')
zipFile.close()

os.listdir('/content/BrainMRI/Datasest Merged 1/Datasest Merged 1')

path = '/content/BrainMRI/Datasest Merged 1/Datasest Merged 1'
list_path = os.listdir('/content/BrainMRI/Datasest Merged 1/Datasest Merged 1')
def count_size_image(list,path):
  total = 0
  for i in list:
    folder_path = os.path.join(path,i)
    count_size = len(os.listdir(folder_path))
    print(i," ",count_size)
    total = count_size + total
  return total

jumlah = count_size_image(list_path,path)
print(f'Jumlah Dataset dalam File ini sebanyak {jumlah} File')

def print_each_photo(folder_path):
    subfolders = os.listdir(folder_path)

    for subfolder in subfolders:
        subfolder_path = os.path.join(folder_path, subfolder)
        images = os.listdir(subfolder_path)[:3]
        fig, axes = plt.subplots(1, 3, figsize=(12, 4))
        for ax, image_filename in zip(axes, images):
            img = mpimg.imread(os.path.join(subfolder_path, image_filename))
            ax.imshow(img)
            ax.axis('off')
        fig.suptitle(subfolder, fontsize=16)
        fig.subplots_adjust(top=0.85)
        plt.show()

folder_path = '/content/BrainMRI/Datasest Merged 1/Datasest Merged 1'
print_each_photo(folder_path)

from shutil import copyfile

dataset_folder = "/content/BrainMRI/Datasest Merged 1/Datasest Merged 1"
target_samples = 2500
classes = ["meningioma", "glioma", "notumor", "pituitary"]

def downsample_class(class_name, target_samples):
    class_files = os.listdir(os.path.join(dataset_folder, class_name))
    num_samples_to_keep = min(target_samples, len(class_files))
    samples_to_keep = random.sample(class_files, num_samples_to_keep)
    source_folder = os.path.join(dataset_folder, class_name)
    destination_folder = os.path.join(dataset_folder, "downsampled", class_name)
    os.makedirs(destination_folder, exist_ok=True)
    for file_name in samples_to_keep:
        source_path = os.path.join(source_folder, file_name)
        destination_path = os.path.join(destination_folder, file_name)
        copyfile(source_path, destination_path)

for class_name in classes:
    downsample_class(class_name, target_samples)

path_new = '/content/BrainMRI/Datasest Merged 1/Datasest Merged 1/downsampled'
list_path_new = os.listdir('/content/BrainMRI/Datasest Merged 1/Datasest Merged 1/downsampled')
jumlah = count_size_image(list_path_new,path_new)
print(f'Jumlah Dataset dalam File ini sebanyak {jumlah} File')

"""#### Split Folder Into Training Testing 80:20"""

splitfolders.ratio(path_new,output = "/content/UsedDataset/", seed = 1337, ratio=(0.8, 0.2))

train_size = os.listdir('/content/UsedDataset/train')
train_path = '/content/UsedDataset/train'
test_path = '/content/UsedDataset/val'
test_size = os.listdir('/content/UsedDataset/val')
jumlah_train = count_size_image(train_size,train_path)
jumlah_val = count_size_image(test_size,test_path)

print(f'Jumlah Train dalam Dataset ini sebanyak {jumlah_train} file')
print(f'Jumlah Test dalam Dataset ini sebanyak {jumlah_val} file')

base_dir = '/content/UsedDataset'
train_dir = os.path.join(base_dir, 'train')
validation_dir = os.path.join(base_dir, 'val')

"""### Preprocessing"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    horizontal_flip=True,
    vertical_flip = True,
    zoom_range=0.2,
    shear_range=0.2,
    fill_mode = 'nearest'
    )

val_datagen = ImageDataGenerator(
    rescale=1./255
    )

train_generator = train_datagen.flow_from_directory(
        train_dir,
        target_size=(150, 150),
        batch_size=64,
        shuffle = True,
        class_mode='categorical',
        subset='training'
        )

validation_generator = val_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=64,
        shuffle = True,
        class_mode='categorical'
        )

"""### Model"""

from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, SeparableConv2D, GlobalAveragePooling2D, Flatten, Dense,DepthwiseConv2D
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.models import Sequential
from keras import regularizers
from tensorflow.keras.applications import EfficientNetV2B3

### Next Model
model = Sequential([
    Conv2D(32, (3, 3), activation='swish', input_shape=(150, 150, 3)),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='swish'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='swish',strides=2, padding='same'),
    MaxPooling2D(2, 2),
    BatchNormalization(),
    Dropout(0.1),

    Conv2D(128, (3, 3), activation='swish'),
    BatchNormalization(),
    Conv2D(128, (3, 3), activation='swish'),
    BatchNormalization(),
    Conv2D(256, (3, 3), activation='swish',strides=2, padding='same'),
    MaxPooling2D(2, 2),
    BatchNormalization(),
    Dropout(0.1),

    Conv2D(256, (3, 3), activation='swish'),
    BatchNormalization(),
    Conv2D(512, (3, 3), activation='swish'),
    BatchNormalization(),
    Conv2D(512, (3, 3), activation='swish'),
    MaxPooling2D(2, 2),
    BatchNormalization(),
    Dropout(0.1),

    GlobalAveragePooling2D(),

    Flatten(),
    Dense(512, activation='swish'),
    Dropout(0.5),
    Dense(10, activation='swish'),
    Dense(10, activation='swish'),
    Dense(4, activation='softmax')
])

model.compile(
    optimizer = AdamW(learning_rate = 0.001),
    loss = 'categorical_crossentropy',
    metrics = ['accuracy']
    )

model.summary()

"""### Callback"""

tensorboard = TensorBoard(log_dir='/content/logs')

checkpoint_path = "Brain_Tumor"
checkpoint_callback = ModelCheckpoint(checkpoint_path,
                                      save_weights_only=True,
                                      monitor="val_accuracy",
                                      save_best_only=True)

reduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 3, min_delta = 0.1,
                              mode='auto',verbose=1)

class MyCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.93):
      print("\nAkurasi telah mencapai > 93%!")
      self.model.stop_training = True

on_stop_accuracy = MyCallback()

history = model.fit(
    train_generator,
    epochs=150,
    validation_data=validation_generator,
    verbose = 2,
    callbacks=[
        checkpoint_callback,
        reduce_lr,
        tensorboard,
        on_stop_accuracy
    ]
)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Akurasi Model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss Model')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir logs

print(history.history)

import pathlib
export_dir = '/content/saved_model/'
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('submission.tflite')
tflite_model_file.write_bytes(tflite_model)