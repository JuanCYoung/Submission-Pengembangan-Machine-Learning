# -*- coding: utf-8 -*-
"""submission.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aUx-CmGcWhlq3ZHy7xGNBSa2pbgP7f44
"""

import pandas as pd
from google.colab import files
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import os
import zipfile
import re, string
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix

"""link dataset : https://www.kaggle.com/datasets/tariqsays/sentiment-dataset-with-1-million-tweets"""

!pip install -q kaggle

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list

!kaggle datasets download -d tariqsays/sentiment-dataset-with-1-million-tweets

zipPath = '/content/sentiment-dataset-with-1-million-tweets.zip'
zipFile = zipfile.ZipFile(zipPath, 'r')
zipFile.extractall('/content/Dataset')
zipFile.close()

df = pd.read_csv('/content/Dataset/dataset.csv')

df.head()

df_en_only = df[df['Language'] == 'en']

df_en_only

df_en_only.Label.value_counts()

df_en_only.isna().sum()

random_samples = {}
labels = ['positive', 'negative', 'uncertainty', 'litigious']
sample_size = 1000

for label in labels:
    label_data = df_en_only[df_en_only['Label'] == label]
    random_sample = label_data.sample(n=sample_size, random_state=42)
    random_samples[label] = random_sample

result_df = pd.concat(random_samples.values(), ignore_index=True)
print(result_df)

df_shuffled = result_df.sample(frac=1, random_state=42)
df_shuffled = df_shuffled.reset_index(drop = True)

df_shuffled

category = pd.get_dummies(df_shuffled.Label)
df_baru = pd.concat([df_shuffled.Text, category],axis = 1)

df_baru.head(10)

"""###Deep Cleaning Teks"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def remove_stop_words(df):
    word_tokens = word_tokenize(df)
    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]
    return ' '.join(filtered_sentence)

def strip_all_entities(text):
    text = text.replace('\r', '').replace('\n', ' ').replace('\n', ' ').lower()
    text = re.sub(r"(?:\@|https?\://)\S+", "", text)
    text = re.sub(r'[^\x00-\x7f]',r'', text)
    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'
    table = str.maketrans('', '', banned_list)
    text = text.translate(table)
    return text

def clean_hashtags(text):
    words = re.split('#(?!(?:hashtag)\b)[\w-]+(?=(?:\s+#[\w-]+)*\s*$)', text)
    cleaned_text = " ".join(word.strip() for word in words)
    cleaned_text = " ".join(word.strip() for word in re.split('#|_', cleaned_text))
    return cleaned_text

def filter_chars(a):
    sent = []
    for word in a.split(' '):
        if ('$' in word) | ('&' in word):
            sent.append('')
        else:
            sent.append(word)
    return ' '.join(sent)

def remove_mult_spaces(text):
    return re.sub("\s\s+" , " ", text)

df_clean = df_baru
df_clean['Text'] = df_clean['Text'].apply(strip_all_entities)
df_clean['Text'] = df_clean['Text'].apply(clean_hashtags)
df_clean['Text'] = df_clean['Text'].apply(filter_chars)
df_clean['Text'] = df_clean['Text'].apply(remove_mult_spaces)
df_clean['Text'] = df_clean['Text'].apply(remove_stop_words)

df_clean.head()

"""### Ubah Nilai pada DataFrame"""

text_df = df_clean['Text'].values
label = df_clean[['positive', 'negative', 'uncertainty', 'litigious']].values

X_train, X_val, y_train, y_val = train_test_split(text_df,label, test_size = .2,random_state = 101)

"""### Tokenisasi Data"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
import tensorflow_hub as hub

tokenizer = Tokenizer(num_words= 3000, oov_token='')
tokenizer.fit_on_texts(X_train)
tokenizer.fit_on_texts(X_val)

sekuens_latih = tokenizer.texts_to_sequences(X_train)
sekuens_test = tokenizer.texts_to_sequences(X_val)

padded_latih = pad_sequences(
    sekuens_latih,
    maxlen = 200,
    padding='post'
    )
padded_test = pad_sequences(
    sekuens_test,
    maxlen = 200,
    padding='post'
    )

class MyCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('val_accuracy')>0.92):
      print("\nAkurasi telah mencapai > 92%!")
      self.model.stop_training = True

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(
        input_dim=3000,
        output_dim=300,
        input_length = 200,
    ),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(4, activation='softmax')
])

model.compile(loss='categorical_crossentropy',optimizer=Adam(learning_rate=0.001),metrics=['accuracy'])

on_stop = MyCallback()

checkpoint_path = "Sentiment_NLP"
checkpoint_callback = ModelCheckpoint(checkpoint_path,
                                      save_weights_only=True,
                                      monitor="val_accuracy",
                                      save_best_only=True)

auto_reduce_lr = ReduceLROnPlateau(
    monitor = 'val_accuracy',
    patience = 3,
    factor = 0.1,
    min_lr = 0.00001
)

history = model.fit(padded_latih, y_train,
                    epochs=100,
                    validation_data=(padded_test, y_val),
                    verbose=2,
                    callbacks = [on_stop,checkpoint_callback,auto_reduce_lr])

import matplotlib.pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.legend()
plt.show

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Metrics')
plt.legend()
plt.show